{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Prisoner's Dilemma\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "The [Prisoner's Dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) (PD) is a classical game analyzed in game theory, which is widely used to (attempt to) model social/economical interaction. It's a \"dilemma\" as, if exploited to explain the emergence of altruism in human or in general animal society, it fails badly at a first glance.\n",
    "\n",
    "The classical situation-representation of the PD is that of two prisoners whose conviction depends on their mutual cooperation. It is easier understood though if illustrated in terms of a trade-off game (closed bag exachange):\n",
    "\n",
    "*Two people meet and exchange closed bags, with the understanding that one of them contains money, and the other contains a purchase. Either player can choose to honor the deal by putting into his or her bag what he or she agreed, or he or she can defect by handing over an empty bag.*\n",
    "\n",
    "It is obvious that for both players the winning strategy is to NOT cooperate.\n",
    "\n",
    "Things changes when the interaction between the two individuals is iterated, in that case a more altruist attitude (strategy) is expected to emerge. The goal of this project is to test this hypothesis.\n",
    "\n",
    "Mathematically the PD can be expressed with very basic linear algebra. The key component is the **Payoff matrix** $M$, which quantify the reward each player gets depending on whether she cooperated or not (defect):\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{pmatrix} \n",
    "R & S \\\\\n",
    "T & P \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with $T,R,S,P$ integers that satisfy the following conditions:\n",
    "\n",
    "$$\n",
    "T>R>P>S; \\quad 2R > T+S\n",
    "$$\n",
    "\n",
    "for example $T=3$, $R=2$, $P=1$ and $S=0$, or  $T=5$, $R=3$, $P=2$, $S=0$. Each player choice (move) can be represented by one of the two axis in ${\\rm I\\!R}^2$, i.e. $u_C=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ or $u_D=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, where the first coordinate stands for *Cooperate* and the second for *Defect*. Being $u_1$ and $u_2$ their rewards $r_1$ and $r_2$ can be computed then as:\n",
    "\n",
    "$$\n",
    "r_1 = u_1^T M u_2\n",
    "\\quad\n",
    "\\quad\n",
    "r_2 = u_2^T M u_1\n",
    "$$\n",
    "\n",
    "In an Iterative Prisoner's Dilemma (IPD), two players play prisoner's dilemma more than once in succession and they remember previous actions of their opponent and change their strategy accordingly. The winning strategy is the one which yields to a larger reward at the end of the IPD.\n",
    "\n",
    "The strategy can be represented as a function which outputs either $u_C$ or $u_D$. Such function can depend on the opponent's history of moves, her on history of moves, on the number of moves played till that moment and so on, but it can only be based on a probability density function. Possible strategies are:\n",
    "\n",
    "* **Nice guy**: always cooperate (the function's output is always $u_D$)\n",
    "* **Bad guy**: always defect \n",
    "* **Mainly nice**: randomly defect $k\\%$ of the times and cooperate $100-k\\%$, $k<50$\n",
    "* **Mainly bad**: randomly defect $k\\%$ of the times and cooperate $100-k\\%$, $k>50$\n",
    "* **tit-for-tat**: start by cooperating, then repeat what the opponent has done in the previous move \n",
    "\n",
    "Many more and much more complex strategies can be implemented. The strategy can even change during the IPD.\n",
    "\n",
    "\n",
    "### Assignments\n",
    "\n",
    "* Implement a simple IPD between two players implementing two given strategies. Study the evolution along the tournament confronting different strategies; study the overall outcome in the different configurations. \n",
    "* Implement a multiple players IPD (MPIPD) where several strategies play against each other in a roud-robin scheme\n",
    "* Iterate what done in the previous task (repeated MPIPD, rMPIPD)  by increasing the population implementing a given strategy depending on the results that strategy achieved in the previous iteration\n",
    "* (*difficult*) Implement a rMPIPD where strategies are allowed to mutate. The goal is to simulate the effect of genetic mutations and the effect of natura selection. A parameter (gene) should encode the attidue of an individual to cooperate, such gene can mutate randomly and the corresponding phenotype should compete in the MPIPD such that the best-fitted is determined.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Player(ABC):\n",
    "    \"\"\"Abstract class, declare the move method\"\"\"\n",
    "    @abstractmethod\n",
    "    def move(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiceGuy(Player):\n",
    "    \"\"\"\n",
    "    Player that always outputs the cooperate move\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[]):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history  \n",
    "        \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "        \n",
    "    def move(self):\n",
    "        uc = [1,0]\n",
    "        self.move_history.append(uc)\n",
    "        return uc\n",
    "\n",
    "    \n",
    "class BadGuy(Player):\n",
    "    \"\"\"\n",
    "    Player that always outputs the defect move\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[]):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history\n",
    "        \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "        \n",
    "    def move(self):\n",
    "        ud = [0,1]\n",
    "        self.move_history.append(ud)\n",
    "        return ud\n",
    "\n",
    "    \n",
    "class KBadGuy(Player):\n",
    "    \"\"\"\n",
    "    Player that outputs the defect move \n",
    "    with probability k/100\n",
    "    \"\"\"\n",
    "    def __init__(self, k, r_history=[], move_history=[]):\n",
    "        self.k = k\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history\n",
    "        \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "        \n",
    "    def move(self):\n",
    "        uc = [1,0]\n",
    "        ud = [0,1]\n",
    "        if np.random.rand() > 1-(self.k/100):\n",
    "            u = ud\n",
    "        else:\n",
    "            u = uc        \n",
    "        self.move_history.append(u)\n",
    "        return u\n",
    " \n",
    "    \n",
    "class Tit4Tat(Player):\n",
    "    \"\"\"\n",
    "    Player, the first move is cooperate, \n",
    "    the subsequent moves are the same as the last \n",
    "    element in self.input_history\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[], input_history=[]):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history \n",
    "        self.input_history = input_history\n",
    "\n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "        \n",
    "    def append_input(self, inp):\n",
    "        self.input_history.append(inp)\n",
    "        \n",
    "    def move(self):\n",
    "        if len(self.input_history) == 0:\n",
    "            u = [1,0]\n",
    "        else:\n",
    "            u = self.input_history[-1]\n",
    "        self.move_history.append(u)\n",
    "        return u\n",
    "    \n",
    "    \n",
    "class Tit4TatMP(Player):\n",
    "    \"\"\"\n",
    "    Player that starts with cooperate and then cooperates only if a fraction of players\n",
    "    above or equal c_treshold has cooperated.\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[], input_history=[], c_threshold=0.5):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history \n",
    "        self.input_history = input_history\n",
    "        self.c_threshold = c_threshold\n",
    "    \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "        \n",
    "    def append_input(self, m_inp):\n",
    "        \"\"\"\n",
    "        multiplayer input m_inp should be an array or list with N rows for N players,\n",
    "        in every row the list [1,0] or [0,1], it also works with [1] or [0].\n",
    "        The number of input players can change. \n",
    "        \"\"\"\n",
    "        self.input_history.append(m_inp)\n",
    "        \n",
    "    def move(self):\n",
    "        if len(self.input_history) == 0:\n",
    "            u = [1,0]\n",
    "        else:\n",
    "            last_inp = np.array(self.input_history[-1])[:,0]\n",
    "            if np.sum(last_inp)/np.shape(last_inp)[0] >= self.c_threshold:\n",
    "                u = [1,0]\n",
    "            else:\n",
    "                u = [0,1]\n",
    "        self.move_history.append(u)\n",
    "        return u\n",
    "        \n",
    "        \n",
    "class GrimTriggerMP(Player):\n",
    "    \"\"\"\n",
    "    Player that cooperates until a numbers of players >= d_threshold defect in a turn,\n",
    "    from now on he defects in every turn. \n",
    "    Setting d_threshold=0 means that it needs only one defecting player\n",
    "    to start defecting itself.\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[], input_history=[], d_threshold=0.5):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history \n",
    "        self.input_history = input_history\n",
    "        self.d_threshold = d_threshold\n",
    "        self.defected = False\n",
    "    \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)  \n",
    "        \n",
    "    def append_input(self, m_inp):\n",
    "        \"\"\"\n",
    "        multiplayer input m_inp should be an array or list with N rows for N players,\n",
    "        in every row the list [1,0] or [0,1], it also works with [1] or [0].\n",
    "        The number of input players can change. \n",
    "        \"\"\"\n",
    "        self.input_history.append(m_inp)\n",
    "    \n",
    "    def move(self):  \n",
    "        if not self.defected and len(self.input_history) > 0:\n",
    "            # check if it has been defected in the last turn\n",
    "            last_inp = last_inp = np.array(self.input_history[-1])[:,0]\n",
    "            if np.sum(last_inp)/np.shape(last_inp)[0] < 1 - self.d_threshold:\n",
    "                self.defected = True\n",
    "#                 print(\"debug DEFECTED\")\n",
    "                \n",
    "        if not self.defected:\n",
    "            u = [1,0]\n",
    "        else:\n",
    "            u = [0,1]\n",
    "        self.move_history.append(u)\n",
    "        return u\n",
    "\n",
    "    \n",
    "    \n",
    "class LearningPlayer(Player):\n",
    "    \"\"\"\n",
    "    Player that use past rewards in order to decide the next move\n",
    "    \"\"\"\n",
    "    def __init__(self, r_history=[], move_history=[]):\n",
    "        self.r_history = r_history\n",
    "        self.move_history = move_history \n",
    "    \n",
    "    def append_reward(self, reward):\n",
    "        self.r_history.append(reward)\n",
    "    \n",
    "\n",
    "    def move(self):\n",
    "        # maybe is better to use numpy\n",
    "        \n",
    "        # random move if its the first move\n",
    "        if len(self.r_history) == 0:\n",
    "            if np.random.rand() >= 0.5:\n",
    "                u = [1,0]\n",
    "            else:\n",
    "                u = [0,1]\n",
    "        else:\n",
    "            move_cat_history = [move[0] for move in self.move_history]\n",
    "            move_cat_history = [-1 if move == 0 else 1 for move in move_cat_history]\n",
    "            r_cat_history = []\n",
    "            for i in range(len(self.r_history)):\n",
    "                r_cat_history.append(self.r_history[i] * move_cat_history[i])\n",
    "            if sum(r_cat_history) > 0:\n",
    "                # cooperate if rewards>0\n",
    "                u = [1,0]\n",
    "            elif sum(r_cat_history) < 0:\n",
    "                # defect if rewards<0\n",
    "                u = [0,1]\n",
    "            else:\n",
    "                # random choice if parity\n",
    "                if np.random.rand() >= 0.5:\n",
    "                    u = [1,0]\n",
    "                else:\n",
    "                    u = [0,1]\n",
    "                \n",
    "        self.move_history.append(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of players classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_guy = NiceGuy()\n",
    "\n",
    "for i in range(10):\n",
    "    u = nice_guy.move()\n",
    "print(u)\n",
    "print(nice_guy.move_history)\n",
    "print(nice_guy.r_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pino = KBadGuy(30)\n",
    "for i in range(10):\n",
    "    u = pino.move()\n",
    "print(u)\n",
    "print(pino.move_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t4t_guy = Tit4Tat()\n",
    "u = t4t_guy.move()\n",
    "print(\"first move \",u)\n",
    "\n",
    "print(t4t_guy.move_history)\n",
    "t4t_guy.input_history.append([0,1])\n",
    "u = t4t_guy.move()\n",
    "print(\"second move (mirrored) \", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4t_m = Tit4TatMP([],[],[],0.5)\n",
    "\n",
    "u = t4t_m.move()\n",
    "print(\"first move \", u)\n",
    "\n",
    "m_inp = [[1,0], [0,1], [0,1]]\n",
    "\n",
    "t4t_m.append_input(m_inp)\n",
    "\n",
    "u = t4t_m.move()\n",
    "print(\"second move \", u)\n",
    "\n",
    "m_inp = [[1], [0], [0], [1], [1]]\n",
    "t4t_m.append_input(m_inp)\n",
    "\n",
    "u = t4t_m.move()\n",
    "print(\"third move \", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with d_threshold=0 it needs one defecting player to change\n",
    "grim_trigger = GrimTriggerMP([],[],[],0)\n",
    "u = grim_trigger.move()\n",
    "print(\"first move is cooperate \", u)\n",
    "\n",
    "# one defector over 4 other players\n",
    "m_inp = [[1,0], [1,0], [1,0], [0,1]]\n",
    "grim_trigger.append_input(m_inp)\n",
    "\n",
    "u = grim_trigger.move()\n",
    "print(\"second move \", u)\n",
    "\n",
    "m_inp = [[1], [1], [1], [1]]\n",
    "grim_trigger.append_input(m_inp)\n",
    "\n",
    "u = grim_trigger.move()\n",
    "print(\"third move \", u)\n",
    "\n",
    "u = grim_trigger.move()\n",
    "print(\"fourth move \", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 2 players IPD\n",
    "Implement a simple IPD between two players implementing two given strategies. Study the evolution along the tournament confronting different strategies; study the overall outcome in the different configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KBadGuy vs Tit4Tat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "# for practicity we are using u as horizontal arrays\n",
    "# initialize players instances\n",
    "p1 = Tit4Tat([],[],[])\n",
    "p2 = KBadGuy(40,[],[])\n",
    "\n",
    "# game loop\n",
    "for i in range(100):\n",
    "    u1 = p1.move()\n",
    "    u2 = p2.move()\n",
    "    #append p2 move, necessary for the tit4tat\n",
    "    p1.append_input(u2)\n",
    "    \n",
    "    p1.append_reward(np.array(u1) @ M @ np.array(u2).T)\n",
    "    p2.append_reward(np.array(u2) @ M @ np.array(u1).T)\n",
    "\n",
    "# visualization\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(p1.r_history, label=(\"p1\"))\n",
    "ax[0].plot(p2.r_history, label=(\"p2\"))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"moves\")\n",
    "ax[0].set_ylabel(\"reward\")\n",
    "ax[0].set_title(\"Reward for every move\")\n",
    "\n",
    "ax[1].plot(np.cumsum(p1.r_history), label=(\"p1\"))\n",
    "ax[1].plot(np.cumsum(p2.r_history), label=(\"p2\"))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"moves\")\n",
    "ax[1].set_ylabel(\"cumulative reward\")\n",
    "ax[1].set_title(\"Cumulative reward\")\n",
    "ax[1].grid(linestyle=\":\")\n",
    "\n",
    "\n",
    "print(\"player1 final reward \", sum(p1.r_history))\n",
    "print(\"player2 final reward \", sum(p2.r_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tit4Tat vs Tit4Tat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "# for practicity we are using u as horizontal arrays\n",
    "# initialize players instances\n",
    "p1 = Tit4Tat([],[],[])\n",
    "p2 = Tit4Tat([],[],[])\n",
    "\n",
    "# game loop\n",
    "for i in range(100):\n",
    "    u1 = p1.move()\n",
    "    p2.append_input(u1)\n",
    "    \n",
    "    u2 = p2.move()\n",
    "    p1.append_input(u2)\n",
    "        \n",
    "    p1.append_reward(np.array(u1) @ M @ np.array(u2).T)\n",
    "    p2.append_reward(np.array(u2) @ M @ np.array(u1).T)\n",
    "    \n",
    "# visualization    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(p1.r_history, label=(\"p1\"))\n",
    "ax[0].plot(p2.r_history, label=(\"p1\"))\n",
    "ax[0].set_ylim(0,3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.cumsum(p1.r_history), label=(\"p1\"))\n",
    "ax[1].plot(np.cumsum(p2.r_history), label=(\"p2\"))\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "print(\"player1 final reward \", sum(p1.r_history))\n",
    "print(\"player2 final reward \", sum(p2.r_history))\n",
    "\n",
    "print(\"As expected two Tit4Tat players cooperates and obtain reward 2*i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KBadGuy vs KBadGuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "# initialize players instances\n",
    "p1 = KBadGuy(10,[],[])\n",
    "p2 = KBadGuy(30,[],[])\n",
    "\n",
    "# game loop\n",
    "for i in range(100):\n",
    "    u1 = p1.move()\n",
    "#     p2.append_input(u1)\n",
    "    \n",
    "    u2 = p2.move()\n",
    "#     p1.append_input(u2)\n",
    "        \n",
    "    p1.append_reward(np.array(u1) @ M @ np.array(u2).T)\n",
    "    p2.append_reward(np.array(u2) @ M @ np.array(u1).T)\n",
    "    \n",
    "# visualization\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(p1.r_history, label=(\"p1\"))\n",
    "ax[0].plot(p2.r_history, label=(\"p2\"))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"moves\")\n",
    "ax[0].set_ylabel(\"reward\")\n",
    "ax[0].set_title(\"Reward for every move\")\n",
    "\n",
    "ax[1].plot(np.cumsum(p1.r_history), label=(\"p1\"))\n",
    "ax[1].plot(np.cumsum(p2.r_history), label=(\"p2\"))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"moves\")\n",
    "ax[1].set_ylabel(\"cumulative reward\")\n",
    "ax[1].set_title(\"Cumulative reward\")\n",
    "ax[1].grid(linestyle=\":\")\n",
    "\n",
    "\n",
    "print(\"player1 final reward \", sum(p1.r_history))\n",
    "print(\"player2 final reward \", sum(p2.r_history))\n",
    "\n",
    "print(\"If two KBadGuy are playing, the one with bigger k tends to win\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the entire range of KBadGuys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analize the connection between the two *k* values and the total reward value\n",
    "running multiple games with different *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "k_p1 = np.arange(0, 101, 10)\n",
    "k_schedule = np.zeros((121,2))\n",
    "\n",
    "for i in range(0,121,11):\n",
    "    k_schedule[i:i+11, 0] = k_p1[i//11]\n",
    "    k_schedule[i:i+11, 1] = k_p1[:]\n",
    "\n",
    "\n",
    "reward_history = []\n",
    "for n in range(np.shape(k_schedule)[0]):\n",
    "    reward_game = [0,0]\n",
    "    p1 = KBadGuy(k_schedule[n,0],[],[])\n",
    "    p2 = KBadGuy(k_schedule[n,1],[],[])\n",
    "    for i in range(100):              \n",
    "        u1 = p1.move()\n",
    "        u2 = p2.move()\n",
    "\n",
    "        reward1 = np.array(u1) @ M @ np.array(u2).T\n",
    "        reward2 = np.array(u2) @ M @ np.array(u1).T\n",
    "        p1.append_reward(reward1)\n",
    "        p2.append_reward(reward2)\n",
    "        \n",
    "        reward_game[0] += reward1\n",
    "        reward_game[1] += reward2\n",
    "        \n",
    "    reward_history.append(reward_game)\n",
    "    \n",
    "reward_history = np.asarray(reward_history)\n",
    "reward_2d = np.reshape(reward_history, (11,11,2))\n",
    "\n",
    "# visualization    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(reward_history[:,0], label=(\"p1\"))\n",
    "ax[0].plot(reward_history[:,1], label=(\"p2\"))\n",
    "ax[0].legend()\n",
    "ax[0].grid(linestyle=\":\")\n",
    "ax[0].set_title(\"final rewards for the combinations of k\")\n",
    "ax[1].plot(np.sum(reward_history,1), label=(\"p1\"))\n",
    "ax[1].grid(linestyle=\":\")\n",
    "ax[1].set_title(\"Sum of the final reward for both players\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize = (22,6))\n",
    "im0 = ax[0].imshow(reward_2d[:,:,0], origin='lower')\n",
    "ax[0].set_xticks(range(11))\n",
    "ax[0].set_yticks(range(11))\n",
    "ax[0].set_xticklabels(range(0,110,10))\n",
    "ax[0].set_yticklabels(range(0,110,10))\n",
    "ax[0].set_xlabel(\"p1 k\")\n",
    "ax[0].set_ylabel(\"p2 k\")\n",
    "ax[0].set_title(\"P1 rewards depending on the combination of k\")\n",
    "plt.colorbar(im0, ax=ax[0])\n",
    "\n",
    "im1 = ax[1].imshow(reward_2d[:,:,1], origin='lower')\n",
    "ax[1].set_xticks(range(11))\n",
    "ax[1].set_yticks(range(11))\n",
    "ax[1].set_xticklabels(range(0,110,10))\n",
    "ax[1].set_yticklabels(range(0,110,10))\n",
    "ax[1].set_xlabel(\"p1 k\")\n",
    "ax[1].set_ylabel(\"p2 k\")\n",
    "ax[1].set_title(\"P2 rewards depending on the combination of k\")\n",
    "plt.colorbar(im1, ax=ax[1])\n",
    "\n",
    "im2 = ax[2].imshow(np.sum(reward_2d, 2), origin='lower')\n",
    "ax[2].set_xticks(range(11))\n",
    "ax[2].set_yticks(range(11))\n",
    "ax[2].set_xticklabels(range(0,110,10))\n",
    "ax[2].set_yticklabels(range(0,110,10))\n",
    "ax[2].set_xlabel(\"p1 k\")\n",
    "ax[2].set_ylabel(\"p2 k\")\n",
    "ax[2].set_title(\"Sum of P1, P2 rewards\")\n",
    "plt.colorbar(im2, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Learning Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with learning players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "p1 = KBadGuy(30,[],[])\n",
    "p2 = LearningPlayer([],[])\n",
    "\n",
    "for i in range(100):\n",
    "    u1 = p1.move()\n",
    "    u2 = p2.move()\n",
    "    \n",
    "    p1.append_reward(np.array(u1) @ M @ np.array(u2).T)\n",
    "    p2.append_reward(np.array(u2) @ M @ np.array(u1).T)\n",
    "    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(p1.r_history, label='KBadGuy')\n",
    "ax[0].plot(p2.r_history, label='LearningPlayer')\n",
    "ax[0].set_xlabel(\"moves\")\n",
    "ax[0].set_ylabel(\"reward\")\n",
    "ax[0].set_title(\"Reward for every move\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.cumsum(p1.r_history), label='KBadGuy')\n",
    "ax[1].plot(np.cumsum(p2.r_history), label='LearningPlayer')\n",
    "ax[1].grid(linestyle=\":\")\n",
    "ax[1].set_xlabel(\"moves\")\n",
    "ax[1].set_ylabel(\"cumulative reward\")\n",
    "ax[1].set_title(\"Cumulative reward\")\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "print(\"KBadGuy final reward \", sum(p1.r_history))\n",
    "print(\"LearningPlayer final reward \", sum(p2.r_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Player vs Learning Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "p1 = LearningPlayer([],[])\n",
    "p2 = LearningPlayer([],[])\n",
    "\n",
    "for i in range(100):\n",
    "    u1 = p1.move()\n",
    "    u2 = p2.move()\n",
    "    \n",
    "    p1.append_reward(np.array(u1) @ M @ np.array(u2).T)\n",
    "    p2.append_reward(np.array(u2) @ M @ np.array(u1).T)\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(p1.r_history, label='LearningPlayer1')\n",
    "ax[0].plot(p2.r_history, label='LearningPlayer2')\n",
    "ax[0].set_xlabel(\"moves\")\n",
    "ax[0].set_ylabel(\"reward\")\n",
    "ax[0].set_title(\"Reward for every move\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.cumsum(p1.r_history), label='LearningPlayer1')\n",
    "ax[1].plot(np.cumsum(p2.r_history), label='LearningPlayer2')\n",
    "ax[1].grid(linestyle=\":\")\n",
    "ax[1].set_xlabel(\"moves\")\n",
    "ax[1].set_ylabel(\"cumulative reward\")\n",
    "ax[1].set_title(\"Cumulative reward\")\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "print(\"KBadGuy final reward \", sum(p1.r_history))\n",
    "print(\"LearningPlayer final reward \", sum(p2.r_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning player against every k bad player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "k_p1 = np.arange(0, 101, 10)\n",
    "k_schedule = np.arange(101)\n",
    "# k_schedule = np.flip(k_schedule)\n",
    "\n",
    "reward_history = []\n",
    "for n in range(np.shape(k_schedule)[0]):\n",
    "    reward_game = [0,0]\n",
    "    p1 = KBadGuy(k_schedule[n],[],[])\n",
    "    p2 = LearningPlayer([],[])\n",
    "   \n",
    "    for i in range(100):          \n",
    "        u1 = p1.move()\n",
    "        u2 = p2.move()\n",
    "\n",
    "        reward1 = np.array(u1) @ M @ np.array(u2).T\n",
    "        reward2 = np.array(u2) @ M @ np.array(u1).T\n",
    "        p1.append_reward(reward1)\n",
    "        p2.append_reward(reward2)\n",
    "        \n",
    "        reward_game[0] += reward1\n",
    "        reward_game[1] += reward2\n",
    "        \n",
    "    reward_history.append(reward_game)\n",
    "    \n",
    "reward_history = np.asarray(reward_history)\n",
    "# reward_2d = np.reshape(reward_history, (10,10,2))\n",
    "\n",
    "# print(reward_history)\n",
    "# visualization    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(reward_history[:,0], label=(\"KBadGuy\"))\n",
    "ax[0].plot(reward_history[:,1], label=(\"LearningPlayer\"))\n",
    "ax[0].legend()\n",
    "ax[0].grid(linestyle=\":\")\n",
    "ax[0].set_title(\"final rewards for the combinations of k\")\n",
    "ax[1].plot(np.sum(reward_history,1), label=(\"p1\"))\n",
    "ax[1].grid(linestyle=\":\")\n",
    "ax[1].set_title(\"Sum of the final reward for both players\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning player against every k bad player \n",
    "multiple run in order to reduce the randomness effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff matrix\n",
    "M = np.array([[2,0],[3,1]])\n",
    "\n",
    "k_p1 = np.arange(0, 101, 10)\n",
    "k_schedule = np.arange(101)\n",
    "\n",
    "\n",
    "reward_history = []\n",
    "games_per_batch = 100\n",
    "for n in range(np.shape(k_schedule)[0]):\n",
    "    reward_game_batch = [0,0]\n",
    "    for j in range(games_per_batch):\n",
    "        reward_game = [0,0]\n",
    "        p1 = KBadGuy(k_schedule[n],[],[])\n",
    "        p2 = LearningPlayer([],[])\n",
    "        for i in range(100):      \n",
    "            u1 = p1.move()\n",
    "            u2 = p2.move()\n",
    "\n",
    "            reward1 = np.array(u1) @ M @ np.array(u2).T\n",
    "            reward2 = np.array(u2) @ M @ np.array(u1).T\n",
    "            p1.append_reward(reward1)\n",
    "            p2.append_reward(reward2)\n",
    "\n",
    "            reward_game[0] += reward1\n",
    "            reward_game[1] += reward2\n",
    "            \n",
    "        reward_game_batch[0] += reward_game[0]\n",
    "        reward_game_batch[1] += reward_game[1]\n",
    "\n",
    "    reward_history.append(reward_game_batch)\n",
    "    \n",
    "reward_history = np.asarray(reward_history)/games_per_batch\n",
    "\n",
    "\n",
    "\n",
    "# visualization    \n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "ax[0].plot(reward_history[:,0], label=(\"KBadGuy\"))\n",
    "ax[0].plot(reward_history[:,1], label=(\"LearningPlayer\"))\n",
    "ax[0].legend()\n",
    "ax[0].grid(linestyle=\":\")\n",
    "ax[0].set_title(\"final rewards for the combinations of k\")\n",
    "ax[1].plot(np.sum(reward_history,1))\n",
    "ax[1].grid(linestyle=\":\")\n",
    "ax[1].set_title(\"Sum of the final reward for both players\")\n",
    "\n",
    "print(\"we can see that a LearningPlayer obtain a higher or similar final reward playing every KBadGuy\")\n",
    "print(\"So it wins against a NiceGuy, a MainlyNiceGuy, and has similar results to a MainlyBadGuy and a BadGuy\")\n",
    "print(\"notice that the results are very noisy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: multiplayer IPD\n",
    "\n",
    "\n",
    "In order to extend our game to a N-Players scenario, we started from the normal form matrix proposed in [1]:\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{pmatrix} \n",
    "R & K & S \\\\\n",
    "T & L & P \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "where the first collumn identifies the case in which all the N-1 players decide to collaborate (strategy $(C...C)$), the second one the case in which at least one player decides to defect (strategy $(C...D)$), and the last one represent the case where every N-1 player chooses to defect (strategy $(D...D)$).\n",
    "<br><br>\n",
    "Furthermore, the strategy D has to continue to be the dominant one, thus we added few more constraint on the values of $R,K,S,T,L,P$:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "T > R > L > K > P > S\\\\\n",
    "2R > T+S\\\\\n",
    "2K > S+L\n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "These constraints lead to the following payoff matrix that will be used for the rest of the project:\n",
    "<br><br>\n",
    "$$\n",
    "M = \n",
    "\\begin{pmatrix} \n",
    "7 & 3 & 0 \\\\\n",
    "9 & 5 & 1 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "Thanks to the linear algebra formulation of the problem, we won't need to define the players payoffs for every combination in the outcome set $\\{C,D\\}^N$.<br>\n",
    "In fact, the player $i$ payoff can be easly computed as:\n",
    "$$\n",
    "r_i = u_i^T M u_{N-1}\n",
    "$$\n",
    "<br>\n",
    "where $u_{N-1}$ is defined as:\n",
    "\n",
    "$$\n",
    "u_{N-1} =\n",
    "\\begin{cases}\n",
    "[1,0,0]\\quad \\textrm{if all the N-1 players play C}\\\\\n",
    "[0,1,0]\\quad \\textrm{if at least one of the N-1 players plays D}\\\\\n",
    "[0,0,1]\\quad \\textrm{if all the N-1 players play D}\n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "When we refer to an IPD, new strategies can be defined such as:\n",
    "* **tit-for-tat-MPIPD**: start by cooperating, then repeat what the majority of the opponets has done in the previous move.\n",
    "* **Grim-Trigger guy**: start cooperating and keep cooperating till the other does too. If the other defects, start defecting from now on, no matter what the other does.\n",
    "\n",
    "\n",
    "\n",
    "From game theory we know that the last round will be played selfishly and a Nash Equilibrium will be played from rational players; thus the last outcome will be $D...D$ (every player chooses to defect), leading to a low payoff equal to 1.\n",
    "However it is evident that, if the number of game iterations $T$ is greater than $T_{th} = \\frac{14}{6}$, cooperating till the last but one round would be the best strategy given that:\n",
    "\n",
    "$$\n",
    "(T-1)*(C...C)+(D...D) > (C...D) + (T-1)(D...D)\\\\\n",
    "7(T-1)+1 > 9 + (T-1)\\\\\n",
    "\\textrm{that with:}\\quad T=3>T_{th}\\quad \\textrm{leads to:}\\quad 15 > 11\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References:<br>\n",
    "[1]: https://www.researchgate.net/publication/295855229_Iterated_symmetric_three-player_prisoner%27s_dilemma_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=9; R=7; L=5; K=3; P=1; S=0;\n",
    "\n",
    "# Payoff matrix\n",
    "M2 = np.array([[R, K, S],[T, L, P]])\n",
    "print(\"M2\")\n",
    "print(M2)\n",
    "\n",
    "\n",
    "\n",
    "#P_list = player list\n",
    "#GI = number of Game Iterations\n",
    "def MPIPD(P_list, GI):\n",
    "    # game loop\n",
    "    for i in range(GI):\n",
    "\n",
    "        U = np.array([P_list[i].move() for i in range(len(P_list))]).T #matrix with all the players move of this round\n",
    "\n",
    "        for k in range(len(P_list)):   #for each player   \n",
    "            #get moves of others player (if CC, CD or DD)\n",
    "            #problems with case k=0 and k=len(P) solved in silly way\n",
    "            if(k!=0 and k!= len(P_list)-1): \n",
    "                others_moves = np.prod(U[:,0:k], axis=1) * np.prod(U[:, k:], axis=1)\n",
    "            elif(k==(len(P_list)-1)):\n",
    "                others_moves = np.prod(U[:,0:k], axis=1)\n",
    "            else:\n",
    "                others_moves = np.prod(U[:,1:], axis=1)\n",
    "\n",
    "\n",
    "            if(np.array(others_moves == [1,0]).all()): #case CC\n",
    "\n",
    "                others_moves = np.array([1,0,0])\n",
    "\n",
    "            elif(np.array(others_moves == [0,0]).all()): #case CD\n",
    "\n",
    "                others_moves = np.array([0,1,0])\n",
    "\n",
    "            else: #case DD\n",
    "\n",
    "                others_moves = np.array([0,0,1])\n",
    "\n",
    "\n",
    "            #check if player is tit4tat, if yes, update input history\n",
    "            if(isinstance(P_list[k], Tit4Tat)):\n",
    "\n",
    "                if(others_moves[0] == 1): #the others played CC\n",
    "                    P_list[k].append_input([1,0])\n",
    "\n",
    "                else: #the others played CD or DD                \n",
    "                    P_list[k].append_input([0,1])\n",
    "\n",
    "            if(isinstance(P_list[k], Tit4TatMP) or isinstance(P_list[k], GrimTriggerMP)):\n",
    "                P_list[k].append_input(np.delete(U, k, axis=1)) #copy all moves but the player i-th one\n",
    "                \n",
    "                \n",
    "\n",
    "            P_list[k].append_reward(U[:,k] @ M2 @ others_moves.T)\n",
    "\n",
    "   \n",
    "\n",
    "            \n",
    "#P_list = player list\n",
    "#P_plot_index = indexes of the player that we want to plot\n",
    "def plot_rewards_MPIPD(P_list, P_plot_index=None):        \n",
    "\n",
    "    if(P_plot_index == None): \n",
    "        P_plot_index = range(len(P_list))\n",
    "        \n",
    "        \n",
    "    fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "\n",
    "    for i in P_plot_index:\n",
    "\n",
    "        print(\"player\"+ str(i) +\" final reward \"+ str(sum(P_list[i].r_history)) +\". Player type: \" + str(type(P_list[i])))\n",
    "        ax[0].plot(P_list[i].r_history)\n",
    "        ax[1].plot(np.cumsum(P_list[i].r_history), label=str(type(P_list[i])))\n",
    "        ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize players instances\n",
    "\n",
    "p1 = Tit4Tat([],[],[])\n",
    "p2 = KBadGuy(40,[],[])\n",
    "p3 = NiceGuy([],[])\n",
    "p4 = BadGuy([],[])\n",
    "p5 = KBadGuy(60,[],[])\n",
    "\n",
    "P = [p1, p2, p3, p4, p5]\n",
    "\n",
    "#run the game\n",
    "MPIPD(P, 100)\n",
    "\n",
    "#plot the results\n",
    "plot_rewards_MPIPD(P)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "p1 = Tit4TatMP([],[],[],0.6)\n",
    "p2 = KBadGuy(60,[],[])\n",
    "p3 = NiceGuy([],[])\n",
    "p4 = GrimTriggerMP([],[],[],0.4)\n",
    "p5 = LearningPlayer([],[])\n",
    "p6 = KBadGuy(30,[],[])\n",
    "p7 = LearningPlayer([],[])\n",
    "\n",
    "P = [p1, p2, p3, p4, p5, p6, p7]\n",
    "\n",
    "MPIPD(P, 50)\n",
    "plot_rewards_MPIPD(P, [0,3,4,5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: reapeted multiplayer IPD with population increasing\n",
    "Iterate what done in the previous task (repeated MPIPD, rMPIPD)  by increasing the population implementing a given strategy depending on the results that strategy achieved in the previous iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4: rMPIPD with mutating strategies\n",
    "(*difficult*) Implement a rMPIPD where strategies are allowed to mutate. The goal is to simulate the effect of genetic mutations and the effect of natura selection. A parameter (gene) should encode the attidue of an individual to cooperate, such gene can mutate randomly and the corresponding phenotype should compete in the MPIPD such that the best-fitted is determined. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
